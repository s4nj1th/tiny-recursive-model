{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tiny Recursive Reasoning Model (TRM)\n",
    "\n",
    "This notebook contains the TRM implementation converted from `main.py`. It includes:\n",
    "- configuration dataclass\n",
    "- model (TinyRecursiveModel) and residual blocks\n",
    "- a flexible `GenericDataset` that can load JSON, NPY, PT and CSV (including the provided `sudoku.csv` with `question`/`answer` columns)\n",
    "- collate function and trainer\n",
    "\n",
    "Notes:\n",
    "- The notebook will not start training automatically. Use the example cell at the end to create dataset / loaders and to run training manually.\n",
    "- Loading 1M sudoku rows into memory may use a lot of RAM. Consider streaming or converting to a binary format if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TRMConfig:\n",
    "    input_dim: int = 256\n",
    "    hidden_dim: int = 512\n",
    "    output_dim: int = 256\n",
    "    L_layers: int = 2\n",
    "    H_cycles: int = 3\n",
    "    L_cycles: int = 6\n",
    "    dropout: float = 0.1\n",
    "\n",
    "    batch_size: int = 32\n",
    "    epochs: int = 100\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_steps: int = 1000\n",
    "\n",
    "    data_dir: str = \"data/\"\n",
    "    train_split: float = 0.8\n",
    "\n",
    "    save_dir: str = \"checkpoints/\"\n",
    "    save_every: int = 10\n",
    "\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TinyRecursiveModel(nn.Module):\n",
    "    def __init__(self, config: TRMConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        self.input_proj = nn.Linear(config.input_dim, config.hidden_dim)\n",
    "\n",
    "        self.latent_layers = nn.ModuleList([\n",
    "            ResidualBlock(config.hidden_dim, config.dropout)\n",
    "            for _ in range(config.L_layers)\n",
    "        ])\n",
    "\n",
    "        self.output_layers = nn.ModuleList([\n",
    "            ResidualBlock(config.hidden_dim, config.dropout) for _ in range(2)\n",
    "        ])\n",
    "\n",
    "        self.output_proj = nn.Linear(config.hidden_dim, config.output_dim)\n",
    "\n",
    "        self.latent_gate = nn.Parameter(torch.ones(1))\n",
    "        self.output_gate = nn.Parameter(torch.ones(1))\n",
    "\n",
    "    def latent_recursion(self, x, y, z):\n",
    "        combined = x + y + z\n",
    "        for layer in self.latent_layers:\n",
    "            combined = combined + self.latent_gate * layer(combined)\n",
    "        return combined\n",
    "\n",
    "    def output_refinement(self, y, z):\n",
    "        combined = y + z\n",
    "        for layer in self.output_layers:\n",
    "            combined = combined + self.output_gate * layer(combined)\n",
    "        return combined\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embedded = self.input_proj(x)\n",
    "        y = torch.zeros_like(x_embedded)\n",
    "        z = torch.zeros_like(x_embedded)\n",
    "\n",
    "        for _ in range(self.config.H_cycles):\n",
    "            for _ in range(self.config.L_cycles):\n",
    "                z = self.latent_recursion(x_embedded, y, z)\n",
    "            y = self.output_refinement(y, z)\n",
    "\n",
    "        return self.output_proj(y)\n",
    "\n",
    "    def get_num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericDataset(Dataset):\n",
    "    \"\"\"Load JSON, NPY, PT/PTH and CSV (special handling for Sudoku CSV)\"\"\"\n",
    "    def __init__(self, data_dir: str, split: str = \"train\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.split = split\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _parse_sudoku_string(self, s: Optional[str]):\n",
    "        if s is None:\n",
    "            return None\n",
    "        s = ''.join(ch for ch in str(s) if ch.isdigit())\n",
    "        if len(s) != 81:\n",
    "            return None\n",
    "        arr = np.array([int(ch) for ch in s], dtype=np.int32)\n",
    "        return arr.reshape(9, 9)\n",
    "\n",
    "    def _load_data(self) -> List[Dict]:\n",
    "        data: List[Dict] = []\n",
    "\n",
    "        json_files = list(self.data_dir.glob(\"*.json\"))\n",
    "        for json_file in json_files:\n",
    "            with open(json_file, \"r\") as f:\n",
    "                file_data = json.load(f)\n",
    "                if isinstance(file_data, list):\n",
    "                    data.extend(file_data)\n",
    "                else:\n",
    "                    data.append(file_data)\n",
    "\n",
    "        npy_files = list(self.data_dir.glob(\"*.npy\"))\n",
    "        for npy_file in npy_files:\n",
    "            arr = np.load(npy_file, allow_pickle=True)\n",
    "            if arr.dtype == object:\n",
    "                data.extend(arr.tolist())\n",
    "            else:\n",
    "                data.append({\"input\": arr, \"target\": arr})\n",
    "\n",
    "        pt_files = list(self.data_dir.glob(\"*.pt\")) + list(self.data_dir.glob(\"*.pth\"))\n",
    "        for pt_file in pt_files:\n",
    "            tensor_data = torch.load(pt_file)\n",
    "            if isinstance(tensor_data, dict):\n",
    "                data.append(tensor_data)\n",
    "            elif isinstance(tensor_data, (list, tuple)):\n",
    "                data.extend([{\"input\": item, \"target\": item} for item in tensor_data])\n",
    "\n",
    "        csv_files = list(self.data_dir.glob(\"*.csv\"))\n",
    "        for csv_file in csv_files:\n",
    "            with open(csv_file, \"r\", newline=\"\") as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                if reader.fieldnames:\n",
    "                    q_col = None\n",
    "                    a_col = None\n",
    "                    for fn in reader.fieldnames:\n",
    "                        ln = fn.lower()\n",
    "                        if ln in (\"question\", \"quiz\", \"input\", \"puzzle\", \"q\"):\n",
    "                            q_col = fn\n",
    "                        if ln in (\"answer\", \"solution\", \"target\", \"sol\", \"a\"):\n",
    "                            a_col = fn\n",
    "                    if q_col and a_col:\n",
    "                        for row in reader:\n",
    "                            quiz = row.get(q_col)\n",
    "                            solution = row.get(a_col)\n",
    "                            q_arr = self._parse_sudoku_string(quiz)\n",
    "                            s_arr = self._parse_sudoku_string(solution)\n",
    "                            if q_arr is not None and s_arr is not None:\n",
    "                                data.append({\"input\": q_arr, \"target\": s_arr})\n",
    "                    else:\n",
    "                        f.seek(0)\n",
    "                        for line in f:\n",
    "                            line = line.strip()\n",
    "                            if not line:\n",
    "                                continue\n",
    "                            parts = line.split(\",\")\n",
    "                            if len(parts) < 2:\n",
    "                                continue\n",
    "                            quiz, solution = parts[0].strip(), parts[1].strip()\n",
    "                            q_arr = self._parse_sudoku_string(quiz)\n",
    "                            s_arr = self._parse_sudoku_string(solution)\n",
    "                            if q_arr is not None and s_arr is not None:\n",
    "                                data.append({\"input\": q_arr, \"target\": s_arr})\n",
    "        print(f\"Loaded {len(data)} samples from {self.data_dir}\")\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "\n",
    "        if isinstance(item, dict):\n",
    "            input_data = item.get(\"input\", item.get(\"question\", item.get(\"x\")))\n",
    "            target_data = item.get(\"target\", item.get(\"answer\", item.get(\"y\")))\n",
    "        else:\n",
    "            input_data = target_data = item\n",
    "\n",
    "        def _to_tensor(data):\n",
    "            if isinstance(data, torch.Tensor):\n",
    "                t = data\n",
    "                try:\n",
    "                    return t.reshape(-1, 1)\n",
    "                except Exception:\n",
    "                    return t\n",
    "            if isinstance(data, np.ndarray):\n",
    "                arr = data\n",
    "            elif isinstance(data, list):\n",
    "                arr = np.array(data)\n",
    "            else:\n",
    "                return torch.FloatTensor([data]).reshape(-1, 1)\n",
    "            if arr.ndim == 2 and arr.shape == (9, 9):\n",
    "                arr = arr.reshape(-1)\n",
    "            return torch.FloatTensor(arr).reshape(-1, 1)\n",
    "\n",
    "        input_data = _to_tensor(input_data)\n",
    "        target_data = _to_tensor(target_data)\n",
    "\n",
    "        return input_data, target_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    inputs, targets = zip(*batch)\n",
    "\n",
    "    max_seq_len = max(x.shape[0] if len(x.shape) > 0 else 1 for x in inputs)\n",
    "\n",
    "    padded_inputs = []\n",
    "    padded_targets = []\n",
    "\n",
    "    for inp, tgt in zip(inputs, targets):\n",
    "        if len(inp.shape) == 1:\n",
    "            inp = inp.unsqueeze(-1)\n",
    "        if len(tgt.shape) == 1:\n",
    "            tgt = tgt.unsqueeze(-1)\n",
    "\n",
    "        if inp.shape[0] < max_seq_len:\n",
    "            pad_size = max_seq_len - inp.shape[0]\n",
    "            inp = F.pad(inp, (0, 0, 0, pad_size))\n",
    "            tgt = F.pad(tgt, (0, 0, 0, pad_size))\n",
    "\n",
    "        padded_inputs.append(inp)\n",
    "        padded_targets.append(tgt)\n",
    "\n",
    "    return torch.stack(padded_inputs), torch.stack(padded_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TRMTrainer:\n",
    "    def __init__(self, model: TinyRecursiveModel, config: TRMConfig):\n",
    "        self.model = model.to(config.device)\n",
    "        self.config = config\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(), lr=config.lr, weight_decay=config.weight_decay\n",
    "        )\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.optimizer, T_max=config.epochs\n",
    "        )\n",
    "\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        os.makedirs(config.save_dir, exist_ok=True)\n",
    "\n",
    "    def train_epoch(self, dataloader):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        pbar = tqdm(dataloader, desc=\"Training\")\n",
    "        for inputs, targets in pbar:\n",
    "            inputs = inputs.to(self.config.device)\n",
    "            targets = targets.to(self.config.device)\n",
    "\n",
    "            outputs = self.model(inputs)\n",
    "\n",
    "            if outputs.shape != targets.shape:\n",
    "                targets = targets[:, : outputs.shape[1], : outputs.shape[2]]\n",
    "\n",
    "            loss = self.criterion(outputs, targets)\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "    def validate(self, dataloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in dataloader:\n",
    "                inputs = inputs.to(self.config.device)\n",
    "                targets = targets.to(self.config.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "\n",
    "                if outputs.shape != targets.shape:\n",
    "                    targets = targets[:, : outputs.shape[1], : outputs.shape[2]]\n",
    "\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        return total_loss / len(dataloader)\n",
    "\n",
    "    def train(self, train_loader, val_loader=None):\n",
    "        print(f\"Training TRM with {self.model.get_num_params():,} parameters\")\n",
    "        print(f\"Device: {self.config.device}\")\n",
    "\n",
    "        best_val_loss = float(\"inf\")\n",
    "\n",
    "        for epoch in range(self.config.epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.config.epochs}\")\n",
    "\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "            if val_loader:\n",
    "                val_loss = self.validate(val_loader)\n",
    "                print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    self.save_checkpoint(\"best.pt\")\n",
    "\n",
    "            self.scheduler.step()\n",
    "\n",
    "            if (epoch + 1) % self.config.save_every == 0:\n",
    "                self.save_checkpoint(f\"epoch_{epoch + 1}.pt\")\n",
    "\n",
    "        print(\"\\nTraining complete!\")\n",
    "\n",
    "    def save_checkpoint(self, filename):\n",
    "        path = os.path.join(self.config.save_dir, filename)\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": self.model.state_dict(),\n",
    "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
    "                \"config\": self.config,\n",
    "            },\n",
    "            path,\n",
    "        )\n",
    "        print(f\"Saved checkpoint: {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: create dataset and a single batch to inspect shapes\n",
    "data_dir = \"data/\"\n",
    "sudoku_csv = os.path.join(data_dir, \"sudoku.csv\")\n",
    "if os.path.exists(sudoku_csv):\n",
    "    print(\"Detected sudoku.csv - will load the CSV (make sure it has 81-digit quiz and solution strings).\")\n",
    "else:\n",
    "    print(\"No sudoku.csv detected in data/. If you have other data formats put them in the data/ folder.\")\n",
    "\n",
    "# Configure for Sudoku if CSV is present\n",
    "if os.path.exists(sudoku_csv):\n",
    "    cfg = TRMConfig(\n",
    "        input_dim=1,\n",
    "        hidden_dim=128,\n",
    "        output_dim=1,\n",
    "        L_layers=2,\n",
    "        H_cycles=3,\n",
    "        L_cycles=6,\n",
    "        batch_size=64,\n",
    "        epochs=1,\n",
    "        lr=1e-4,\n",
    "        data_dir=data_dir,\n",
    "        save_dir=\"checkpoints/\",\n",
    "    )\n",
    "else:\n",
    "    cfg = TRMConfig(data_dir=data_dir)\n",
    "\n",
    "ds = GenericDataset(cfg.data_dir)\n",
    "print(f\"Dataset size: {len(ds)}\")\n",
    "if len(ds) > 0:\n",
    "    inp, tgt = ds[0]\n",
    "    print(\"Sample input shape:\", inp.shape)\n",
    "    print(\"Sample target shape:\", tgt.shape)\n",
    "\n",
    "    # Create a small DataLoader\n",
    "    loader = DataLoader(ds, batch_size=min(8, len(ds)), collate_fn=collate_fn)\n",
    "    batch_inputs, batch_targets = next(iter(loader))\n",
    "    print(\"Batch inputs shape:\", batch_inputs.shape)  # [batch, seq_len, input_dim]\n",
    "    print(\"Batch targets shape:\", batch_targets.shape)\n",
    "\n",
    "    # Model sanity check\n",
    "    model = TinyRecursiveModel(cfg)\n",
    "    print(\"Model parameters:\", model.get_num_params())\n",
    "    out = model(batch_inputs)\n",
    "    print(\"Model output shape:\", out.shape)\n",
    "\n",
    "    # Ready to train: create trainer and run one epoch locally (optional)\n",
    "    # trainer = TRMTrainer(model, cfg)\n",
    "    # trainer.train(loader)\n",
    "else:\n",
    "    print(\"No samples to inspect. Add data to the data/ folder (sudoku.csv or other formats).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps and recommendations\n",
    "\n",
    "- If you want to treat Sudoku as a classification per cell (1-9), change the model's `output_dim` to 9 and use `nn.CrossEntropyLoss` with integer targets in [0..8] or [1..9] mapped appropriately.\n",
    "- For the 1,000,000-row `sudoku.csv` file, consider converting to `.npy` or writing a streaming Dataset that reads CSV rows in `__getitem__` to avoid loading everything into memory at once.\n",
    "- If you'd like, I can add a streaming CSV dataset implementation or change the loss/output for classification.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
